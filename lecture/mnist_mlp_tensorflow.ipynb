{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructors**: Prof. Keith Chugg (chugg@usc.edu) & Prof. B. Keith Jenkins (jenkins@sipi.usc.edu)\n",
    "\n",
    "**Notebook**: Written by Prof. Keith Chugg."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classifier for MNIST (and Fashion MNIST) Using MLP in TensorFlow\n",
    "In this notebook, we will use TensorFlow to train an ANN/MLP for the MNIST/FMNIST datasets we previously explored with an MSE classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.datasets import fashion_mnist, mnist\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the Data\n",
    "First, let's get a function to get the MNIST or FMNIST data.  Previously, we used PyTorch to access the datasets, but TF also has these built in.  So, we should be familiar with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_FASHION_MNIST = False\n",
    "\n",
    "#### get the daatset\n",
    "\n",
    "if USE_FASHION_MNIST:\n",
    "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "    tag_name = 'FashionMNIST'\n",
    "    label_names = [\"top\", \"trouser\", \"pullover\", \"dress\", \"coat\", \"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\"]\n",
    "else:\n",
    "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "    tag_name = 'MNIST'\n",
    "    label_names = [f'{i}' for i in set(test_labels)]\n",
    "\n",
    "# train_images.shape is (60000, 28, 28)\n",
    "#test_images.shape (10000, 28, 28)\n",
    "num_pixels = 28 * 28 \n",
    "train_images = train_images.reshape( (60000, num_pixels) ).astype(np.float32) / 255.0\n",
    "test_images = test_images.reshape( (10000, num_pixels) ).astype(np.float32)  / 255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN/MLP Model Definition\n",
    "In TensorFlow, we need to define the model.  Below, we define an ANN that takes a vector of length 784 as input, then has one hidden layer, followed by a SoftMax output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " images (InputLayer)         [(None, 784)]             0         \n",
      "                                                                 \n",
      " hidden (Dense)              (None, 48)                37680     \n",
      "                                                                 \n",
      " output (Dense)              (None, 10)                490       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,170\n",
      "Trainable params: 38,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## tSome hyper-parameters for our model\n",
    "reg_val = 0.0001\n",
    "hidden_nodes = 48\n",
    "\n",
    "\n",
    "# this uses the Functional API for definning the model\n",
    "nnet_inputs = Input(shape=(num_pixels,), name='images')\n",
    "z = Dense(hidden_nodes, activation='relu', kernel_regularizer=regularizers.l2(reg_val), bias_regularizer=regularizers.l2(reg_val), name='hidden')(nnet_inputs)\n",
    "z = Dense(10, activation='softmax', kernel_regularizer=regularizers.l2(reg_val), bias_regularizer=regularizers.l2(reg_val), name='output')(z)\n",
    "\n",
    "our_first_model = Model(inputs=nnet_inputs, outputs=z)\n",
    "\n",
    "#this will print a summary of the model to the screen\n",
    "our_first_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the `our_first_model.summary()` summary caused a model summary to be printed, this includes the layers and the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "#this will produce a digram of the model -- requires pydot and graphviz installed\n",
    "plot_model(our_first_model, to_file='our_first_model.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_norms = np.linalg.norm(W_hat, axis=0)\n",
    "w_norms = np.linalg.norm(W_hat[1:], axis=0)\n",
    "\n",
    "C = W_hat.shape[1]\n",
    "\n",
    "plt.figure()\n",
    "plt.stem(np.arange(C), w_norms)\n",
    "plt.grid(':')\n",
    "plt.xlabel('class index (m)')\n",
    "plt.ylabel(r'$|| {\\bf w}_m ||$')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating and visualizing the MSE Classifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our multiclass linear regression system, we need to use it to make decisions.  I wrote a function that takese in the weight-vectors (as a 2D array) the augmented data matrix, and will evalute the MSE multiclass classifier.  It will also produce histograms of $g_m({\\bf x})$ conditioned on the true class.  This allows us to see the misclassification rate when a given class is true.  It also shows us visually which classes are easily confused.  \n",
    "\n",
    "I took some time to try to document this reasonably well because you can use this on your homework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_multiclass_histograms(X_aug, W, y, fname, norm_W=False, scale=1, class_names=None):\n",
    "    \"\"\"\n",
    "    Keith Chugg, USC, 2023.\n",
    "\n",
    "    X_aug: shape: (N, D + 1).  Augmented data matrix\n",
    "    W: shape: (D + 1, C).  The matrix of augmented weight-vectors.  W.T[m] is the weight vector for class m\n",
    "    y: length N array with int values with correct classes.  Classes are indexed from 0 up.\n",
    "    fname: a pdf of the histgrams will be saved to filename fname\n",
    "    norm_W: boolean.  If True, the w-vectors for each class are normalized.\n",
    "    scale: use scale < 1 to make the figure smaller, >1 to make it bigger\n",
    "    class_names: pass a list of text, descriptive names for the classes.  \n",
    "\n",
    "    This function takes in the weight vectors for a linear classifier and applied the \"maximum value methd\" -- i.e., \n",
    "    it computes the argmax_m g_m(x), where g_m(x) = w_m^T x to find the decision. For each class, it plots the historgrams \n",
    "    of  g_m(x) when class c is true.  This gives insights into which classes are most easily confused -- i.e., similar to a \n",
    "    confusion matrix, but more information.  \n",
    "\n",
    "    Returns: the overall misclassification error percentage\n",
    "    \"\"\"\n",
    "    if norm_W:\n",
    "       W = W / np.linalg.norm(W[1:], axis=0)\n",
    "    y_soft = X_aug @ W\n",
    "    N, C = y_soft.shape\n",
    "    y_hard = np.argmax(y_soft, axis=1)\n",
    "    error_percent = 100 * np.sum(y != y_hard) / len(y) \n",
    "\n",
    "    fig, ax = plt.subplots(C, sharex=True, figsize=(12 * scale, 4 * C * scale))\n",
    "    y_soft_cs = []\n",
    "    conditional_error_rate = np.zeros(C)\n",
    "    if class_names is None:\n",
    "        class_names = [f'Class {i}' for i in range(C)]\n",
    "    for c_true in range(C):\n",
    "        y_soft_cs.append(X_aug[y == c_true] @ W)\n",
    "        y_hard_c = np.argmax(y_soft_cs[c_true], axis=1)\n",
    "        conditional_error_rate[c_true] = 100 * np.sum(y_hard_c != c_true) / len(y_hard_c)\n",
    "    for c_true in range(C):\n",
    "        peak = -100\n",
    "        for c in range(C):\n",
    "            hc = ax[c_true].hist(y_soft_cs[c_true].T[c], bins = 100, alpha=0.4, label=class_names[c])\n",
    "            peak = np.maximum(np.max(hc[0]), peak)\n",
    "            ax[c_true].legend()\n",
    "            ax[c_true].grid(':')\n",
    "        ax[c_true].text(0, 0.9 * peak, f'True: {class_names[c_true]}\\nConditional Error Rate = {conditional_error_rate[c_true] : 0.2f}%')\n",
    "    if norm_W:\n",
    "        ax[C-1].set_xlabel(r'normalized discriminant function $g_m(x) / || {\\bf w} ||$')\n",
    "    else:\n",
    "        ax[C-1].set_xlabel(r'discriminant function $g_m(x)$')\n",
    "    plt.savefig(fname, bbox_inches='tight')\n",
    "    return error_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate = plot_multiclass_histograms(x_train_aug, W_hat, y_train, f'img/hist_{tag_name}.pdf', scale=1, class_names=label_names)\n",
    "print(f'\\nOverall Misclassification Rate: {error_rate : 0.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not too bad!  For FashionMNIST, we can see that when \"shirt\" is true, it is easily confused for many other classes!\n",
    "\n",
    "Also, if you look above, it is class 5 which has the w-vector with smallest norm (FashionMNIST).  Would it make more sense to maximize ${\\bf w}_m{\\bf x}/ \\| {\\bf w}_m\\|$?  Note that this is the distance to the decision boundary for the impicit one-vs-rest test.  \n",
    "\n",
    "Let's try that out, it's buit into `plot_multiclass_histograms()` function with the `norm_W` optional argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate = plot_multiclass_histograms(x_train_aug, W_hat, y_train, f'img/hist_{tag_name}.pdf', norm_W=True, scale=1, class_names=label_names)\n",
    "print(f'\\nOverall Misclassification Rate: {error_rate : 0.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that using `norm_W` as `True` or `False` provides two different decision (or fusion) rules for combining the OvR classifiers.  Which do you think performs better?\n",
    "\n",
    "We can also argue that there is a real issue with an MSE classifier.\n",
    "\n",
    "For example, given that the target of the true class is $+1$, when the true class is $k$, if discriminant function is $g({\\bf x}) \\gg +1$ it should be a high confidence decision that class $k$ is true.  However, the squared error loss actually penalizes for this -- e.g., $g({\\bf x}) = +10$ incurs a huge penalty while $g({\\bf x})=+1$ incurs no penalty.  \n",
    "\n",
    "This can be addressed with multiclass logistic regression where the max operation over all $g_m({\\bf x})$ is replaced by a multiclass softmax function.  This is multiclass logistic regression.  This doesn't have a closed form solution and requires gradient descent to solve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mls23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f2b55be8af22e299bb14a990e5306b14c0e04e0c371124ebf94aa533852bd32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
