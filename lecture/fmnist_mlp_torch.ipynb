{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classifier for MNIST (and Fashion MNIST) Using MLP in PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use PyTorch to train an ANN/MLP for the FMNIST dataset.\n",
    "\n",
    "This notebook requires torch to be installed. To install this, use `pip3 install torch torchvision torchaudio`. This will work on Nvidia GPU, as well as on an M1/M2 Apple chip. You also need to install `tqdm` which is a package used to provide training progress results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the Data\n",
    "First, let's get the data. Torchvision provides many built-in datasets including Fashion MNIST. Fashion MNIST is a dataset of 28x28 images of fashion items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All datasets are subclasses of torch.utils.data.Dataset i.e, they have __getitem__ and __len__ methods implemented\n",
    "from torchvision import datasets, transforms\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),]) # transforms.ToTensor() converts the image to a tensor and transforms.Normalize() normalizes the tensor\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T-shirt/top': 0,\n",
       " 'Trouser': 1,\n",
       " 'Pullover': 2,\n",
       " 'Dress': 3,\n",
       " 'Coat': 4,\n",
       " 'Sandal': 5,\n",
       " 'Shirt': 6,\n",
       " 'Sneaker': 7,\n",
       " 'Bag': 8,\n",
       " 'Ankle boot': 9}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "image, label = trainset[0] \n",
    "print(image.shape) # torch.Size([1, 28, 28])\n",
    "print(label) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion MNIST dataset comes as train and test sets, of sizes 60,000 and 10,000 respectively.\n",
    "If we want to use a validation set, we need to create it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 50000, Validation set size: 10000, Test set size: 10000\n"
     ]
    }
   ],
   "source": [
    "trainset, valset = torch.utils.data.random_split(trainset, [50000, 10000])\n",
    "# Final sizes are 50000, 10000, 10000\n",
    "print(f'Train set size: {len(trainset)}, Validation set size: {len(valset)}, Test set size: {len(testset)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, data is loaded using data loaders, which are iterators over the dataset.\n",
    "Data Loaders need a batch size, which is the size of the data batch extracted in each iteration.\n",
    "We will create a dataloader for each data split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 32\n",
    "# Shuffle the data at the start of each epoch (only useful for training set)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchsize, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batchsize, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchsize, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN/MLP Model Definition\n",
    "Create a 2-layer neural network. In Pytorch, `nn.Module` is the base class for all neural network modules in PyTorch.\n",
    "Your models should also subclass this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=784, out_features=48, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=48, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Parameters for the model\n",
    "n_pixels = 28 * 28\n",
    "n_classes = 10\n",
    "n_hidden = 48\n",
    "\n",
    "# Define the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_pixels, n_classes, n_hidden): # Define layers in the constructor\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_pixels, n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(n_hidden, n_classes)\n",
    "        \n",
    "    def forward(self, x): # Define forward pass in the forward method\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x # note: no softmax, as this is included in the loss function in PyTorch\n",
    "    \n",
    "model = MLP(n_pixels, n_classes, n_hidden)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-class classification, we use the cross entropy loss function. In pytorch, the input to `nn.CrossEntropyLoss` should be logits not probabilities. The loss function combines softmax with loss calculation for numerical stability. For the optimizer algorithm, we will use Adam, a very popular optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2   ## the learning rate in TF is part of the optimizer.  Default is 1e-2\n",
    "reg_val = 1e-4\n",
    "criterion = nn.CrossEntropyLoss() # includes softmax (for numerical stability)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=reg_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=48, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=48, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the device to use and move model to device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") # MPS acceleration is available on MacOS 12.3+\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "model.to(device) # Move model to device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "Unlike Keras, in Pytorch we need to explicitly write the training functions. This includes iterating over data batches, forward and backward passes, loss caluclation and updating the weights.\n",
    "We will write two functions. One for trianing loops (to update the weights of the model) and one for validation loop (to evaluate performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to call for each training epoch (one complete pass over the training set)\n",
    "def train(model, trainloader, criterion, optimizer, device):\n",
    "    model.train() # set model to training mode\n",
    "    running_loss = 0; running_acc = 0\n",
    "    with tqdm(total=len(trainloader), desc=f\"Train\", unit=\"batch\") as pbar:\n",
    "        for n_batch, (images, labels) in enumerate(trainloader): # Iterate over batches\n",
    "            images, labels = images.to(device), labels.to(device) # Move batch to device\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images) # Forward pass\n",
    "            loss = criterion(output, labels) # Compute loss\n",
    "            loss.backward() # Backward pass\n",
    "            optimizer.step() # Update weights\n",
    "            running_loss += loss.item()\n",
    "            running_acc += (output.argmax(1) == labels).float().mean().item()\n",
    "            pbar.set_postfix({'loss': loss.item(), 'acc': 100. * running_acc / (n_batch+1)})\n",
    "            pbar.update() # Update progress bar\n",
    "    return running_loss / len(trainloader), running_acc / len(trainloader) # return loss and accuracy for this epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to call for each validation epoch (one complete pass over the validation set)\n",
    "def validate(model, valloader, criterion, device):\n",
    "    model.eval() # set model to evaluation mode (e.g. turn off dropout, batchnorm, etc.)\n",
    "    running_loss = 0; running_acc = 0\n",
    "    with torch.no_grad(): # no need to compute gradients for validation\n",
    "        with tqdm(total=len(valloader), desc=f\"Eval\", unit=\"batch\") as pbar:\n",
    "            for n_batch, (images, labels) in enumerate(valloader): # Iterate over batches\n",
    "                images, labels = images.to(device), labels.to(device) # Move batch to device\n",
    "                output = model(images) # Forward pass\n",
    "                loss = criterion(output, labels) # Compute loss\n",
    "                running_loss += loss.item() \n",
    "                running_acc += (output.argmax(1) == labels).float().mean().item()\n",
    "                pbar.set_postfix({'loss': loss.item(), 'acc': 100. * running_acc / (n_batch+1)})\n",
    "                pbar.update() # Update progress bar\n",
    "    return running_loss / len(valloader), running_acc / len(valloader)  # return loss and accuracy for this epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we loop over the number of epochs. In each epoch we loop once over the training dataset and update the model weights. Then we loop once over the validation dataset and calculate validation loss and accuracy. We will use tqdm to display the progress bar. We also save the model that acheived the best validation accuracy across all epochs.\n",
    "\n",
    " Note: the training loss/acc displayed are running averages during training while the validation loss/acc are computed after training epoch is finished. This is why val loss might be lower than training loss in the first few epochs. To compare train vs val peformance on the same model, we can recompute training loss/acc after epoch is finished. This takes extra time so we usually ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1563/1563 [00:11<00:00, 135.84batch/s, loss=0.622, acc=76.1]\n",
      "Eval: 100%|██████████| 313/313 [00:01<00:00, 182.85batch/s, loss=0.567, acc=80.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1563/1563 [00:11<00:00, 141.97batch/s, loss=0.446, acc=82.7]\n",
      "Eval: 100%|██████████| 313/313 [00:01<00:00, 185.68batch/s, loss=0.54, acc=83.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1563/1563 [00:10<00:00, 144.46batch/s, loss=0.348, acc=84.1] \n",
      "Eval: 100%|██████████| 313/313 [00:01<00:00, 186.10batch/s, loss=0.529, acc=84.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1563/1563 [00:11<00:00, 137.25batch/s, loss=0.247, acc=85.1]\n",
      "Eval: 100%|██████████| 313/313 [00:02<00:00, 130.55batch/s, loss=0.51, acc=84.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1563/1563 [00:10<00:00, 147.07batch/s, loss=0.697, acc=85.6]\n",
      "Eval: 100%|██████████| 313/313 [00:01<00:00, 160.54batch/s, loss=0.461, acc=85.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1563/1563 [00:10<00:00, 145.76batch/s, loss=0.637, acc=86.1]\n",
      "Eval: 100%|██████████| 313/313 [00:01<00:00, 195.83batch/s, loss=0.494, acc=85.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1563/1563 [00:11<00:00, 139.15batch/s, loss=0.176, acc=86.5]\n",
      "Eval: 100%|██████████| 313/313 [00:01<00:00, 186.43batch/s, loss=0.486, acc=86.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1563/1563 [00:10<00:00, 144.62batch/s, loss=0.447, acc=86.9]\n",
      "Eval: 100%|██████████| 313/313 [00:01<00:00, 187.20batch/s, loss=0.459, acc=86.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1563/1563 [00:10<00:00, 144.33batch/s, loss=0.241, acc=87.2] \n",
      "Eval: 100%|██████████| 313/313 [00:01<00:00, 169.40batch/s, loss=0.444, acc=86.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1563/1563 [00:11<00:00, 136.32batch/s, loss=0.625, acc=87.4] \n",
      "Eval: 100%|██████████| 313/313 [00:01<00:00, 200.12batch/s, loss=0.42, acc=86.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1563/1563 [00:10<00:00, 145.92batch/s, loss=0.71, acc=87.5]  \n",
      "Eval: 100%|██████████| 313/313 [00:01<00:00, 196.90batch/s, loss=0.428, acc=86.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1563/1563 [00:10<00:00, 145.76batch/s, loss=0.261, acc=87.7] \n",
      "Eval: 100%|██████████| 313/313 [00:01<00:00, 185.01batch/s, loss=0.431, acc=86.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1563/1563 [00:11<00:00, 140.19batch/s, loss=0.282, acc=88]   \n",
      "Eval: 100%|██████████| 313/313 [00:01<00:00, 185.08batch/s, loss=0.436, acc=86.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1563/1563 [00:11<00:00, 139.11batch/s, loss=0.297, acc=88.1] \n",
      "Eval: 100%|██████████| 313/313 [00:01<00:00, 195.11batch/s, loss=0.422, acc=87.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1563/1563 [00:15<00:00, 103.57batch/s, loss=0.21, acc=88.4]  \n",
      "Eval: 100%|██████████| 313/313 [00:01<00:00, 184.17batch/s, loss=0.469, acc=87.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1563/1563 [00:10<00:00, 155.23batch/s, loss=0.656, acc=88.6] \n",
      "Eval: 100%|██████████| 313/313 [00:01<00:00, 185.14batch/s, loss=0.454, acc=87.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1563/1563 [00:10<00:00, 144.41batch/s, loss=0.361, acc=88.6] \n",
      "Eval: 100%|██████████| 313/313 [00:01<00:00, 184.22batch/s, loss=0.418, acc=87.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1563/1563 [00:11<00:00, 140.10batch/s, loss=0.236, acc=88.9] \n",
      "Eval: 100%|██████████| 313/313 [00:01<00:00, 181.03batch/s, loss=0.433, acc=87.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 of 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  35%|███▍      | 540/1563 [00:03<00:07, 143.01batch/s, loss=0.342, acc=89.1] "
     ]
    }
   ],
   "source": [
    "# Run training and validation loop\n",
    "# Save the best model based on validation accuracy\n",
    "n_epochs = 30\n",
    "best_acc = -1\n",
    "train_loss_history = []; train_acc_history = []\n",
    "val_loss_history = []; val_acc_history = []\n",
    "for epoch in range(n_epochs): # Iterate over epochs\n",
    "    print(f\"Epoch {epoch+1} of {n_epochs}\")\n",
    "    train_loss, train_acc  = train(model, trainloader, criterion, optimizer, device) # Train\n",
    "    val_loss, val_acc = validate(model, valloader, criterion, device) # Validate\n",
    "    train_loss_history.append(train_loss); train_acc_history.append(train_acc)\n",
    "    val_loss_history.append(val_loss); val_acc_history.append(val_acc)\n",
    "    if val_acc > best_acc: # Save best model\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pt\") # saving model parameters (\"state_dict\") saves memory and is faster than saving the entire model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Training/Validation (Learning Curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = torch.arange(n_epochs)\n",
    "\n",
    "# plot training and validation loss\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_loss_history, label='train_loss')\n",
    "plt.plot(epochs, val_loss_history, label='val_loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Multiclass Cross Entropy Loss')\n",
    "plt.title(f'Loss with{n_hidden} Hidden')\n",
    "plt.legend()\n",
    "\n",
    "# plot training and validation accuracy\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_acc_history, label='train_acc')\n",
    "plt.plot(epochs, val_acc_history, label='val_acc')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Accuracy with {n_hidden} Hidden; Regularizer: {reg_val : 3.2g}')\n",
    "plt.legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model and evaluate on test set\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "test_loss, test_acc = validate(model, testloader, criterion, device)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform inference on a single image form the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # set model to evaluation mode \n",
    "\n",
    "img = np.random.randint(10000)\n",
    "image, label = testset[img] # get first image and label from test set\n",
    "image = image.to(device)  # move image to device\n",
    "output = model(image) # forward pass\n",
    "pred = output.argmax(1) # get predicted class\n",
    "print(f\"Test Image Number: {img}, Predicted class: {testset.classes[pred.item()]}\")\n",
    "# plot image \n",
    "plt.figure()\n",
    "plt.imshow(image.cpu().numpy().squeeze(), cmap='gray')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spert-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
